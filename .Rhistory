store.theta[i,]=theta
store.phi[i,]=phi
store.z[i,]=z
}
plot(store.loglikel,type='l')
store.loglikel
MAP1<- which(store.loglikel==max(store.loglikel))  #107 iteration is MAP
tbsp.clust<- store.z[MAP1[1],]
length(unique(tbsp.clust))
tbsp.clust<- store.z[MAP1[2],]
length(unique(tbsp.clust))
nclustmax=10
z=sample(1:nclustmax,size=nobs,replace=T)
theta=matrix(1/nloc,nclustmax,nloc)
phi=rep(1/nclustmax,nclustmax)
#store results
ngibbs=1000
store.phi=matrix(NA,ngibbs,nclustmax)
store.z=matrix(NA,ngibbs,nobs)
store.theta=matrix(NA,ngibbs,nclustmax*nloc)
store.loglikel=matrix(NA,ngibbs,1)
#gibbs sampler
nburn=ngibbs/2
for (i in 1:ngibbs){
print(i)
#occasionally re-order this
if (i<nburn & i%%50==0){
ind=order(phi,decreasing=T)
theta=theta[ind,]
phi=phi[ind]
znew=z
for (j in 1:nclustmax){
znew[z==ind[j]]=j
}
z=znew
}
#draw samples from FCD's
z=sample.z(dat=dat,theta=theta,phi=phi,
nobs=nobs,nclustmax=nclustmax,nloc=nloc,z=z,n=n)
# z=z.true
v=sample.v(z=z,nclustmax=nclustmax,gamma1=gamma1)
phi=GetPhi(vec=c(v,1),nclustmax=nclustmax)
theta=sample.theta(dat=dat,nclustmax=nclustmax,nloc=nloc,z=z,psi=psi)
#to avoid numerical issues
theta[theta<lo]=lo
# theta=theta.true
#get logl
tmp=sum(dat*log(theta)[z,])+sum(dbeta(v,1,gamma1,log=T))+sum((psi-1)*log(theta))
#store results
store.loglikel[i]=tmp
store.theta[i,]=theta
store.phi[i,]=phi
store.z[i,]=z
}
plot(store.loglikel,type='l')
MAP1<- which(store.loglikel==max(store.loglikel))  #107 iteration is MAP
max(store.loglikel)
MAP1[1]
tbsp.clust<- store.z[MAP1[1],]
time.seg<- 1:nobs
tbsp.clust<- cbind(tbsp.clust,time.seg) %>% data.frame()
tbsp.clust$tbsp.clust<- as.factor(tbsp.clust$tbsp.clust)
levels(tbsp.clust$tbsp.clust)<- 1:length(levels(tbsp.clust$tbsp.clust))
dat1$time.seg<- as.factor(dat1$time.seg)
tbsp.clust$time.seg<- as.factor(tbsp.clust$time.seg)
dat1<- dat1[,1:13]
dat1<- left_join(dat1, tbsp.clust, by="time.seg")
colnames(dat)=1:ncol(dat)
obs1.breakpts<- data.frame(breaks=obs1.breakpts)
obs1.long<- dat %>% data.frame() %>% gather(key, value) %>% mutate(time=rep(1:nobs, times=nloc))
obs1.long$key<- as.factor(obs1.long$key)
levels(obs1.long$key)<- 1:nloc
obs1.long$key<- as.numeric(obs1.long$key)
tbsp.clust[,1]<- tbsp.clust[,1] %>% as.numeric()
tbsp.clust[,2]<- tbsp.clust[,2] %>% as.numeric()
rect.lims<- rle(tbsp.clust$tbsp.clust)
rect.lims$lengths<- cumsum(rect.lims$lengths)+0.5
rect.lims$lengths<- c(0.5, rect.lims$lengths)
rect.lims.new<- matrix(0, length(rect.lims$values), 3)
for (i  in 2:length(rect.lims$lengths)) {
rect.lims.new[i-1,]<- c(rect.lims$lengths[i-1], rect.lims$lengths[i], rect.lims$values[i-1])
}
colnames(rect.lims.new)<- c("xmin","xmax","tbsp.clust")
rect.lims.new<- data.frame(rect.lims.new)
ggplot() +
geom_tile(data=obs1.long, aes(x=time, y=key, fill=log10(value+1))) +
scale_fill_viridis_c("log10(N+1)") +
scale_y_continuous(breaks = 1:6, expand = c(0,0)) +
scale_x_continuous(expand = c(0,0)) +
new_scale_fill() +
geom_vline(data = rect.lims.new, aes(xintercept = xmin), color = "white", size = 0.35) +
geom_rect(data=rect.lims.new, aes(xmin = xmin, xmax = xmax, ymin = 6.5,
ymax = 6.75, fill = tbsp.clust), color = NA, size = 1.5) +
scale_fill_gradientn("Time Cluster", colours = ocean.amp(6)) +
labs(x = "Time Segment", y = "Spatial Cluster") +
theme_bw() +
theme(axis.title = element_text(size = 18), axis.text = element_text(size = 16))
setwd("~/Documents/Snail Kite Project/git_segmentation_behavior")
source('gibbs functions2.R')
source('helper functions.R')
source('gibbs sampler2.R')
dat<- read.csv("Snail Kite Gridded Data.csv", header = T, sep = ",")
names(dat)[7]<- "dist"  #change to generic form
behav.list<- behav.prep(dat=dat, tstep = 3600)  #add move params and filter by 3600 s interval
dat1.res<- behav.gibbs.sampler(behav.list$`1`,50000)
dat=read.csv('ID1 Breakpoints (Behavior).csv',header =T, sep = ",")
View(dat)
behav.heat<- behav.seg.image(behav.list$`1`)
behav.heat
store.theta[i,]=theta
store.theta=matrix(NA,ngibbs,nclustmax*nloc)
for (i in 1:ngibbs){
print(i)
#occasionally re-order this
if (i<nburn & i%%50==0){
ind=order(phi,decreasing=T)
theta=theta[ind,]
phi=phi[ind]
znew=z
for (j in 1:nclustmax){
znew[z==ind[j]]=j
}
z=znew
}
#draw samples from FCD's
z=sample.z(dat=dat,theta=theta,phi=phi,
nobs=nobs,nclustmax=nclustmax,nloc=nloc,z=z,n=n)
# z=z.true
v=sample.v(z=z,nclustmax=nclustmax,gamma1=gamma1)
phi=GetPhi(vec=c(v,1),nclustmax=nclustmax)
theta=sample.theta(dat=dat,nclustmax=nclustmax,nloc=nloc,z=z,psi=psi)
#to avoid numerical issues
theta[theta<lo]=lo
# theta=theta.true
#get logl
tmp=sum(dat*log(theta)[z,])+sum(dbeta(v,1,gamma1,log=T))+sum((psi-1)*log(theta))
#store results
store.loglikel[i]=tmp
store.theta[i,]=theta
store.phi[i,]=phi
store.z[i,]=z
}
nclustmax=10
z=sample(1:nclustmax,size=nobs,replace=T)
theta=matrix(1/nloc,nclustmax,nloc)
phi=rep(1/nclustmax,nclustmax)
#store results
ngibbs=1000
store.phi=matrix(NA,ngibbs,nclustmax)
store.z=matrix(NA,ngibbs,nobs)
store.theta=matrix(NA,ngibbs,nclustmax*nloc)
store.loglikel=matrix(NA,ngibbs,1)
#gibbs sampler
nburn=ngibbs/2
for (i in 1:ngibbs){
print(i)
#occasionally re-order this
if (i<nburn & i%%50==0){
ind=order(phi,decreasing=T)
theta=theta[ind,]
phi=phi[ind]
znew=z
for (j in 1:nclustmax){
znew[z==ind[j]]=j
}
z=znew
}
#draw samples from FCD's
z=sample.z(dat=dat,theta=theta,phi=phi,
nobs=nobs,nclustmax=nclustmax,nloc=nloc,z=z,n=n)
# z=z.true
v=sample.v(z=z,nclustmax=nclustmax,gamma1=gamma1)
phi=GetPhi(vec=c(v,1),nclustmax=nclustmax)
theta=sample.theta(dat=dat,nclustmax=nclustmax,nloc=nloc,z=z,psi=psi)
#to avoid numerical issues
theta[theta<lo]=lo
# theta=theta.true
#get logl
tmp=sum(dat*log(theta)[z,])+sum(dbeta(v,1,gamma1,log=T))+sum((psi-1)*log(theta))
#store results
store.loglikel[i]=tmp
store.theta[i,]=theta
store.phi[i,]=phi
store.z[i,]=z
}
dat=obs.kmeans
dat=dat[which(apply(dat,1,sum)>10),]
n=rowSums(dat)
nobs=nrow(dat)
nloc=ncol(dat)
lo=0.000000000000001
#priors
psi=0.01
gamma1=0.1
#starting values
nclustmax=10
z=sample(1:nclustmax,size=nobs,replace=T)
theta=matrix(1/nloc,nclustmax,nloc)
phi=rep(1/nclustmax,nclustmax)
#store results
ngibbs=1000
store.phi=matrix(NA,ngibbs,nclustmax)
store.z=matrix(NA,ngibbs,nobs)
store.theta=matrix(NA,ngibbs,nclustmax*nloc)
store.loglikel=matrix(NA,ngibbs,1)
#gibbs sampler
nburn=ngibbs/2
for (i in 1:ngibbs){
print(i)
#occasionally re-order this
if (i<nburn & i%%50==0){
ind=order(phi,decreasing=T)
theta=theta[ind,]
phi=phi[ind]
znew=z
for (j in 1:nclustmax){
znew[z==ind[j]]=j
}
z=znew
}
#draw samples from FCD's
z=sample.z(dat=dat,theta=theta,phi=phi,
nobs=nobs,nclustmax=nclustmax,nloc=nloc,z=z,n=n)
# z=z.true
v=sample.v(z=z,nclustmax=nclustmax,gamma1=gamma1)
phi=GetPhi(vec=c(v,1),nclustmax=nclustmax)
theta=sample.theta(dat=dat,nclustmax=nclustmax,nloc=nloc,z=z,psi=psi)
#to avoid numerical issues
theta[theta<lo]=lo
# theta=theta.true
#get logl
tmp=sum(dat*log(theta)[z,])+sum(dbeta(v,1,gamma1,log=T))+sum((psi-1)*log(theta))
#store results
store.loglikel[i]=tmp
store.theta[i,]=theta
store.phi[i,]=phi
store.z[i,]=z
}
plot(store.loglikel,type='l')
MAP1<- which.max(store.loglikel)  #107 iteration is MAP
abline(v=MAP1, col="red")
plot(store.phi[ngibbs,],type='h')
plot(store.theta[ngibbs,],type='h')
plot(store.z[ngibbs,],type='h')
new.theta=matrix(store.theta[ngibbs,], nclustmax, nloc)
image(new.theta)
psi=0.01
gamma1=0.1
#starting values
nclustmax=nobs
z=sample(1:nclustmax,size=nobs,replace=T)
theta=matrix(1/nloc,nclustmax,nloc)
phi=rep(1/nclustmax,nclustmax)
#store results
ngibbs=1000
store.phi=matrix(NA,ngibbs,nclustmax)
store.z=matrix(NA,ngibbs,nobs)
store.theta=matrix(NA,ngibbs,nclustmax*nloc)
store.loglikel=matrix(NA,ngibbs,1)
#gibbs sampler
nburn=ngibbs/2
for (i in 1:ngibbs){
print(i)
#occasionally re-order this
if (i<nburn & i%%50==0){
ind=order(phi,decreasing=T)
theta=theta[ind,]
phi=phi[ind]
znew=z
for (j in 1:nclustmax){
znew[z==ind[j]]=j
}
z=znew
}
#draw samples from FCD's
z=sample.z(dat=dat,theta=theta,phi=phi,
nobs=nobs,nclustmax=nclustmax,nloc=nloc,z=z,n=n)
# z=z.true
v=sample.v(z=z,nclustmax=nclustmax,gamma1=gamma1)
phi=GetPhi(vec=c(v,1),nclustmax=nclustmax)
theta=sample.theta(dat=dat,nclustmax=nclustmax,nloc=nloc,z=z,psi=psi)
#to avoid numerical issues
theta[theta<lo]=lo
# theta=theta.true
#get logl
tmp=sum(dat*log(theta)[z,])+sum(dbeta(v,1,gamma1,log=T))+sum((psi-1)*log(theta))
#store results
store.loglikel[i]=tmp/4
store.theta[i,]=theta
store.phi[i,]=phi
store.z[i,]=z
}
plot(store.loglikel,type='l')
MAP1<- which.max(store.loglikel)  #107 iteration is MAP
abline(v=MAP1, col='red')
store.loglikel[MAP1]
nclustmax=nobs
z=sample(1:nclustmax,size=nobs,replace=T)
theta=matrix(1/nloc,nclustmax,nloc)
phi=rep(1/nclustmax,nclustmax)
#store results
ngibbs=1000
store.phi=matrix(NA,ngibbs,nclustmax)
store.z=matrix(NA,ngibbs,nobs)
store.theta=matrix(NA,ngibbs,nclustmax*nloc)
store.loglikel=matrix(NA,ngibbs,1)
#gibbs sampler
nburn=ngibbs/2
for (i in 1:ngibbs){
print(i)
#occasionally re-order this
if (i<nburn & i%%50==0){
ind=order(phi,decreasing=T)
theta=theta[ind,]
phi=phi[ind]
znew=z
for (j in 1:nclustmax){
znew[z==ind[j]]=j
}
z=znew
}
#draw samples from FCD's
z=sample.z(dat=dat,theta=theta,phi=phi,
nobs=nobs,nclustmax=nclustmax,nloc=nloc,z=z,n=n)
# z=z.true
v=sample.v(z=z,nclustmax=nclustmax,gamma1=gamma1)
phi=GetPhi(vec=c(v,1),nclustmax=nclustmax)
theta=sample.theta(dat=dat,nclustmax=nclustmax,nloc=nloc,z=z,psi=psi)
#to avoid numerical issues
theta[theta<lo]=lo
# theta=theta.true
#get logl
tmp=sum(dat*log(theta)[z,])+sum(dbeta(v,1,gamma1,log=T))+sum((psi-1)*log(theta))
#store results
store.loglikel[i]=tmp/10
store.theta[i,]=theta
store.phi[i,]=phi
store.z[i,]=z
}
plot(store.loglikel,type='l')
MAP1<- which.max(store.loglikel)  #107 iteration is MAP
store.loglikel[MAP1]
set.seed(1)
nclustmax=nobs
z=sample(1:nclustmax,size=nobs,replace=T)
theta=matrix(1/nloc,nclustmax,nloc)
phi=rep(1/nclustmax,nclustmax)
#store results
ngibbs=1000
store.phi=matrix(NA,ngibbs,nclustmax)
store.z=matrix(NA,ngibbs,nobs)
store.theta=matrix(NA,ngibbs,nclustmax*nloc)
store.loglikel=matrix(NA,ngibbs,1)
#gibbs sampler
nburn=ngibbs/2
for (i in 1:ngibbs){
print(i)
#occasionally re-order this
if (i<nburn & i%%50==0){
ind=order(phi,decreasing=T)
theta=theta[ind,]
phi=phi[ind]
znew=z
for (j in 1:nclustmax){
znew[z==ind[j]]=j
}
z=znew
}
#draw samples from FCD's
z=sample.z(dat=dat,theta=theta,phi=phi,
nobs=nobs,nclustmax=nclustmax,nloc=nloc,z=z,n=n)
# z=z.true
v=sample.v(z=z,nclustmax=nclustmax,gamma1=gamma1)
phi=GetPhi(vec=c(v,1),nclustmax=nclustmax)
theta=sample.theta(dat=dat,nclustmax=nclustmax,nloc=nloc,z=z,psi=psi)
#to avoid numerical issues
theta[theta<lo]=lo
# theta=theta.true
#get logl
tmp=sum(dat*log(theta)[z,])+sum(dbeta(v,1,gamma1,log=T))+sum((psi-1)*log(theta))
#store results
store.loglikel[i]=tmp/10
store.theta[i,]=theta
store.phi[i,]=phi
store.z[i,]=z
}
MAP1<- which.max(store.loglikel)  #107 iteration is MAP
store.loglikel[MAP1]
which(store.loglikel==max(store.loglikel))
MAP1<- which(store.loglikel==max(store.loglikel))  #107 iteration is MAP
setwd("~/Documents/Snail Kite Project/Data/R Scripts/cluster_tsegments_loc")
rm(list=ls(all=TRUE))
set.seed(1)
library('Rcpp')
library('MCMCpack')
library(dplyr)
library(ggplot2)
library(tidyr) #for gather function
library(ggnewscale) #for multiple fill scales in ggplot2
library(pals) # for more color palettes
sourceCpp('aux1.cpp')
source('gibbs functions.R') #for clustering
dat<- read.csv("Snail Kite Gridded Data.csv", header = T, sep = ",")
names(dat)[7]<- "dist"  #change to generic form
behav.list<- behav.prep(dat=dat, tstep = 3600)  #add move params and filter by 3600 s interval
source('helper functions.R')
behav.list<- behav.prep(dat=dat, tstep = 3600)  #add move params and filter by 3600 s interval
behav.heat1<- behav.seg.image(behav.list$`1`)
head(behav.heat1)
source('helper functions.R')
behav.breakpts1<- read.csv('ID1 Breakpoints (Behavior).csv', header = T, sep = ',')
behav.breakpts<- read.csv('ID1 Breakpoints (Behavior).csv', header = T, sep = ',')
behav.heat<- behav.seg.image(behav.list$`1`)
dat<- assign.behav.seg(behav.breakpts, behav.list$`1`)
behav.breakpts<- behav.breakpts[,1]
dat<- assign.behav.seg(behav.breakpts, behav.list$`1`)
View(dat)
length(unique(dat$behav.seg))
View(dat)
dat %>% filter(behav.seg==1) %>% group_by(SL) %>% count()
dat %>% filter(behav.seg==1) %>% count()
i=1
dat %>% filter(behav.seg==i) %>% group_by(TA) %>% count()
dat %>% filter(behav.seg==i) %>% group_by(TA) %>% count(!is.na(.))
dat %>% filter(behav.seg==i) %>% group_by(TA) %>% count(!is.na())
dat %>% filter(behav.seg==i) %>% group_by(TA) %>% count() %>% !is.na(.)
dat %>% filter(behav.seg==i) %>% group_by(TA) %>% count() %>% !is.na()
ind=dat %>% filter(behav.seg==i) %>% group_by(TA) %>% count()
ind[!is.na(ind$TA),"TA"]
ind[!is.na(ind$TA),]
in
ind
ind=ind[!is.na(ind$TA),]
ind
dat %>% filter(behav.seg==i) %>% group_by(TAA) %>% count()
ind=dat %>% filter(behav.seg==i) %>% group_by(TAA) %>% count()
ind=ind[!is.na(ind$TAA),]
ind
res.SL[i,ind$SL]=ind$n #takes count of each bin within given time segment
res.SL=matrix(0,n,max.SL)
res.TA=matrix(0,n,max.TA)
res.TAA=matrix(0,n,2)
max.SL=max(dat$SL)
max.TA=max(dat$TA)
max.TA=max(dat$TA, na.rm = T)
res.SL=matrix(0,n,max.SL)
res.TA=matrix(0,n,max.TA)
n=length(unique(dat$behav.seg))
res.SL=matrix(0,n,max.SL)
res.TA=matrix(0,n,max.TA)
res.TAA=matrix(0,n,2)
ind=dat %>% filter(behav.seg==i) %>% group_by(SL) %>% count()
res.SL[i,ind$SL]=ind$n #takes count of each bin within given time segment
View(res.SL)
ind=dat %>% filter(behav.seg==i) %>% group_by(TA) %>% count()
ind=ind[!is.na(ind$TA),]
res.TA[i,ind$TA]=ind$n
View(res.TA)
ind=dat %>% filter(behav.seg==i) %>% group_by(TAA) %>% count()
ind=ind[!is.na(ind$TAA),]
res.TAA[i,ind$TAA]=ind$n
ind
View(res.TAA)
ind$n
ind$TAA
res.TAA[i,]=ind$n
colnames(res.SL)=1:max.SL
colnames(res.TA)=1:max.TA
colnames(res.TAA)=c('n1','n0')
colnames(res.TAA)=c('n0','n1')
View(res.SL)
View(res.TA)
View(res.TAA)
get.summary.stats_behav=function(dat,max.SL,max.TA){  #dat must have SL/TA/TAA assigned by obs
SL=dat$SL
TA=dat$TA
TAA=dat$TAA
n=length(unique(dat$behav.seg))
res.SL=matrix(0,n,max.SL)
res.TA=matrix(0,n,max.TA)
res.TAA=matrix(0,n,2)
for (i in 1:n){
#get SL results
ind=dat %>% filter(behav.seg==i) %>% group_by(SL) %>% count()
res.SL[i,ind$SL]=ind$n #takes count of each bin within given time segment
#get TA results
ind=dat %>% filter(behav.seg==i) %>% group_by(TA) %>% count()
ind=ind[!is.na(ind$TA),]
res.TA[i,ind$TA]=ind$n
#get TAA results
ind=dat %>% filter(behav.seg==i) %>% group_by(TAA) %>% count()
ind=ind[!is.na(ind$TAA),]
res.TAA[i,]=ind$n
}
colnames(res.SL)=1:max.SL
colnames(res.TA)=1:max.TA
colnames(res.TAA)=c('n0','n1')
list(res.TA=res.TA,res.SL=res.SL,res.TAA=res.TAA)
}
get.summary.stats_behav(dat,max.SL,max.TA)
max.SL=max(dat$SL, na.rm = T)
max.TA=max(dat$TA, na.rm = T)
behav.breakpts<- read.csv('ID1 Breakpoints (Behavior).csv', header = T, sep = ',')
behav.breakpts<- behav.breakpts[,1]
dat<- assign.behav.seg(behav.breakpts, behav.list$`1`)
dat<- get.summary.stats_behav(dat,max.SL,max.TA)
str(dat)
class(dat[[1]])
rowSums(behav.heat[[1]])
rowSums(dat[[1]])
rowSums(dat[[2]])
rowSums(dat[[3]])
